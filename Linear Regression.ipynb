{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions: Linear Regression \n",
    "\n",
    "Classification isn't the only task within the scope of supervised machine learning. **Regression** is another equally important task with many complementary features. In regression, we are asked to make a real-valued prediction based upon a dataset of samples. As in classification, we can think of our dataset as an $N\\times P$ matrix, with $N$ samples and $P$ features. However, for supervised tasks, instead of binary/multi-valued *labels* corresponding to partitions of the dataset, we are, often, interested in matching real-valued *response* variables. \n",
    "\n",
    "In the case of simple linear regression, the problems we look at are of the form\n",
    "$$\\mathbf{y} = \\mathbf{X}\\mathbf{w} + \\mathbf{\\xi},$$\n",
    "where $\\mathbf{y}$ are our response variables, the matrix $\\mathbf{X}$ is our dataset, the vector $\\mathbf{\\xi}$ represents some noise or uncertainty on our model, and we hope to find the *explanatory* variables $\\mathbf{w}$ which are able to match our observations and which generalize to future observations. Additionally, one can generalize to a wide class of so-called generalized linear models via\n",
    "$$\\mathbf{y} = {\\rm f}(\\mathbf{X}\\mathbf{w}),$$\n",
    "where $f(\\cdot)$ is often some stochastic function.\n",
    "\n",
    "Here, we will consider solving regressions via **maximum a posteriori (MAP)** approaches. From the Bayesian perspective we are often attempting to solve the problem\n",
    "$${\\rm arg~min}\\quad -\\log P(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}) - \\log P(\\mathbf{w}).$$ \n",
    "If we return to our first definition of linear regression, with additive noise that is assumed to be *iid* Gaussian, and assuming a uniform prior on the value of $\\mathbf{w}$, we arrive at the common least squares (LSQ) problem,\n",
    "$${\\rm arg~min}_{\\mathbf{w}} \\quad ||\\mathbf{y} - \\mathbf{X} \\mathbf{w} ||_2^2,$$\n",
    "which we can interpret as a **maximum likelihood (ML)** solution.\n",
    "\n",
    "Regressions, and specifically linear regressions, often form the basis of the ___\"Do The Simplest Thing That Could Possibly Work.\"___ Many times in production, simple regressions provide good-enough performance which maximize the tradeoff against engineering costs (coding time, computer time). And, as we will later see, regressions can even be applied to solve classification tasks, as in *logistic* regression. So, truly, regressions should be the first step when encountering a new data-science or ML problem. Things can only, hopefully, go up from here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example\n",
    "\n",
    "For example, let's think of the very simplest problem. How about a univariate problem,  \n",
    "$$ y_i = \\alpha x_i + \\xi,$$\n",
    "where the value of $x$ are our known data points, we observe some response variables $y$, and wish to estimate the scalar value of $\\alpha$ which relates the two. To keep this problem from being entirely trivial, we introduce some Gaussian noise $\\xi$ with variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "alpha = 2       # Model parameter to be learned\n",
    "n = 10          # Number of samples to train with\n",
    "var = 1.        # Noise variance\n",
    "\n",
    "# Generate data\n",
    "x = 2 * np.random.rand(n) - 1\n",
    "beta = np.sqrt(var) * np.random.randn(n)\n",
    "y = alpha * x + beta\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(x, y, 'ob')\n",
    "plt.xlabel(\"Datum\", fontsize=16)\n",
    "plt.ylabel(\"Response Variable\", fontsize=16)\n",
    "plt.axis([-2, 2, -4, 4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we estimate the response variable $\\alpha$? In this case it should be quite easy to find the right value of $\\alpha$, especially when we have a lot of data and the value of $\\sigma^2$ is not too high. Lets make a calculation on paper of what this value should be in a Bayesian sense. First we write the likelihood,\n",
    "$$ P(\\mathbf{y} | \\mathbf{x}, \\alpha) = \\frac{1}{(2\\pi \\sigma^2)^{N/2}} \\exp\\left[-\\frac{(\\mathbf{y} - \\alpha \\mathbf{x})^2}{2 \\sigma^2}\\right].$$\n",
    "Now, for MAP estimation we want to look at the negative log value and we can drop any constant terms...\n",
    "\n",
    "$$-\\log P(\\mathbf{y} | \\mathbf{x}, \\alpha) \\propto (\\mathbf{y} - \\alpha \\mathbf{x})^2 \n",
    "=\\mathbf{y}^T\\mathbf{y} + \\alpha^2 \\mathbf{x}^T\\mathbf{x} - 2 \\alpha \\mathbf{y}^T\\mathbf{x}$$\n",
    "\n",
    "Now we have the problem\n",
    "$${\\rm arg~min}_\\alpha \\quad -\\log P(\\mathbf{y}|\\mathbf{x}, \\alpha)\n",
    "= {\\rm arg~min}_\\alpha \\quad \\mathbf{y}^T\\mathbf{y} + \\alpha ^2\\mathbf{x}^T\\mathbf{x} -\n",
    "2 \\alpha \\mathbf{y}^T\\mathbf{x}\n",
    "= {\\rm arg~min}_\\alpha \\quad \\alpha^2\\mathbf{x}^T\\mathbf{x} - 2 \\alpha \\mathbf{y}^T\\mathbf{x}$$\n",
    "\n",
    "Since this is a convex problem over $\\alpha$, taking the minimizer is as simple as finding the root of the derivative, hence,\n",
    "$$\\alpha^* = {\\rm arg~min}_\\alpha \\quad \\alpha^2\\mathbf{x}^T\\mathbf{x} - 2\\alpha\\mathbf{y}^T\\mathbf{x} = \\frac{\\mathbf{y}^T\\mathbf{x}}{\\mathbf{x^T}\\mathbf{x}}$$\n",
    "\n",
    "\n",
    "### Wakeup Task: Calculate the MAP $\\alpha^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# Calculate the MAP estimate of `alpha` given the dataset `x` given observed `y`.\n",
    "alpha_est = \n",
    "print(\"alpha* (ML): \", alpha_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lr1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wakeup-Task: Brute-Force Search\n",
    "\n",
    "*Step 1: Define Likelihood*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(x, y, alpha, var):\n",
    "    \"\"\"\n",
    "        Define the likelihood for a given set of data and expanatory variable $m$,\n",
    "            y = alpha * x + noise (iid Gaussian with variance var)\n",
    "    \"\"\"\n",
    "    l = \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lr2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: Find all values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "alpha_vals = np.linspace(-5, 5, 1000)\n",
    "likelihood_vals = np.zeros(len(alpha_vals))\n",
    "\n",
    "# Loop over values\n",
    "for (i, alpha_test) in enumerate(alpha_vals):\n",
    "    likelihood_vals[i] = likelihood(x, y, alpha_test, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3: Plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot context\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Plot command\n",
    "plt.plot(alpha_vals, likelihood_vals, label = \"likelihood\")\n",
    "\n",
    "# Plot formatting\n",
    "plt.xlim([-5, 5])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"value of $m$\",fontsize=16)\n",
    "plt.ylabel(\"$P(y|F,x,m,\\Delta)$\", fontsize=16)\n",
    "plt.axvline(alpha_est, color=\"k\", linestyle=\":\", label=r\"$\\alpha$ (ML)\")\n",
    "plt.axvline(alpha, color=\"k\", linestyle=\"-\", label=r\"$\\alpha$ (true)\", linewidth = 0.5)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot context\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "# Plot command\n",
    "plt.plot(np.linspace(-5,5,100), alpha_est * np.linspace(-5,5,100), \":k\", linewidth=0.7, label=r\"$\\alpha$ (ML)\")\n",
    "plt.plot(np.linspace(-5,5,100), alpha * np.linspace(-5,5,100), \"-k\", linewidth=0.7, label=r\"$\\alpha$ (true)\")\n",
    "plt.plot(x,y, \"ob\", label = \"data\")\n",
    "\n",
    "# Plot formatting\n",
    "plt.axis([-2, 2, -4, 4])\n",
    "plt.xlabel(\"Datum\",fontsize = 16)\n",
    "plt.ylabel(\"Response\", fontsize = 16)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even for this simple test, when the number of samples is low, we can mismatch the correct value of the slope, even for a quite easy problem like this. \n",
    "\n",
    "## Multiple Features\n",
    "\n",
    "Lets take a look at some models with more than one feature. How can we solve these problems? In the context of linear regression, our features might be linear or non-linear, however the problem remains linear in terms of the predictors. For example, one could attempt to fit a polynomial model, \n",
    "$$ y = w_0 + w_1 x + w_2 x^2 + \\cdots + w_P x^P,$$\n",
    "using a linear regression, e.g.\n",
    "$$ \\mathbf{y} = [\\mathbf{1}, \\mathbf{x}, \\mathbf{x}^2, \\cdots, \\mathbf{x}^P] \\times \\boldsymbol{w} = \\mathbf{X}\\boldsymbol{w}.$$\n",
    "\n",
    "A simple least squares can be used to fit such a model. Lets take a look at an example where the underlying model is an order-2 polynomial (quadratic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem parameters\n",
    "a = -2          # Order 2 coefficient\n",
    "b = 0.1         # Order 1 coefficient\n",
    "c = -30         # Offset\n",
    "n = 100         # Number of samples to train with\n",
    "var = 1.        # Observational noise variance\n",
    "\n",
    "# Generate data\n",
    "x = 6 * np.random.rand(n) - 3\n",
    "noise = np.sqrt(var) * np.random.randn(n)\n",
    "y = a * x ** 2 + b * x + c + noise\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(x,y,'ob')\n",
    "plt.xlabel(\"Datum\",fontsize = 16)\n",
    "plt.ylabel(\"Response Variable\", fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to solve this problem, we need to build the matrix of features. Since we are interested in polynomial fitting, lets write a function which builds the feature matrix for us.\n",
    "\n",
    "### Task: Create Power Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_features(x, max_power):\n",
    "    \"\"\"\n",
    "        Given a vector of data points, x, build a matrix of power\n",
    "        features from 0 (constant) up to power p for use with\n",
    "        polynomial fitting.\n",
    "    \"\"\"\n",
    "    # Initialize data matrix\n",
    "    X = np.zeros((x.shape[0], max_power + 1))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lr3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets make a fit. So, how do we solve for all of the coefficients at once? We can follow through the same as in the simple slope regression example and arrive at the solution for the ML estimate. Another way of looking at this is to see the problem through the lens of linear algebra, we want to find an estimate which minimizes the _residual sum of squares_,\n",
    "\n",
    "$$\\hat{\\boldsymbol{w}} = {\\rm arg~min}_{\\boldsymbol{w}}\\quad{\\rm RSS}(\\boldsymbol{w}) \n",
    "= {\\rm arg~min}_{\\boldsymbol{w}}\\quad ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{w}||_2^2 $$\n",
    "\n",
    "Finding the minimum of this convex cost is simply finding the zero of the gradient, as before,\n",
    "$$ \\frac{\\partial{\\rm RSS}}{\\partial\\boldsymbol{w}} =\n",
    "-2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{w}),$$\n",
    "\n",
    "$$\\therefore\\quad\n",
    "   \\hat{\\boldsymbol{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\n",
    "   \\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "which gives us the classical LSQ solution (as presented in lecture). \n",
    "\n",
    "Now, let us take a look at fitting higher dimensional models to our dataset above. First, we just need to create the different feature sets for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different models\n",
    "X_quad = power_features(x,2)\n",
    "X_cube = power_features(x,3)\n",
    "X_silly = power_features(x,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least-squares directly using linalg.lstsq\n",
    "fit_quad = np.linalg.lstsq(X_quad, y)[0]\n",
    "fit_cube = np.linalg.lstsq(X_cube, y)[0]\n",
    "fit_silly = np.linalg.lstsq(X_silly, y)[0]\n",
    "\n",
    "# Show fit params\n",
    "print(\"Fit quad. Model: \", fit_quad)\n",
    "print(\"Fit cubic Model: \", fit_cube)\n",
    "print(\"Fit silly Model: \", fit_silly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, visualizing what these different models are on our dataset, we see the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fit curve domain\n",
    "xdom = np.linspace(-5, 5, 1000)\n",
    "Xdom = power_features(xdom, 10)\n",
    "\n",
    "# Predictions of different models\n",
    "y_quad = np.dot(Xdom[:, 0:3], fit_quad)\n",
    "y_cube = np.dot(Xdom[:, 0:4], fit_cube)\n",
    "y_silly = np.dot(Xdom, fit_silly)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.plot(x, y,'ob',label = 'Data')\n",
    "plt.plot(xdom, y_quad,':k', label = 'P = 2')\n",
    "plt.plot(xdom, y_cube,'-.k', label = 'P = 3')\n",
    "plt.plot(xdom, y_silly,'--k', label = 'P = 10')\n",
    "plt.plot(xdom, a * xdom ** 2 + b * xdom + c,'-k', linewidth = 0.5, label = 'true model')\n",
    "plt.xlabel(\"Datum\",fontsize = 16)\n",
    "plt.ylabel(\"Response variable\", fontsize = 16)\n",
    "plt.axis([-5, 5, -100, 10])\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that within the range of the data points, with enough presented data, all of the models do well to fit the data. However, outside of those bounds, the error can be much larger. This is the problem of fitting versus generalization. In our case, the error agains the datapoints might be good for higher degree models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training set RSS for different models\n",
    "train_rss_quad = np.mean((y - np.dot(X_quad, fit_quad)) ** 2)\n",
    "train_rss_cube = np.mean((y - np.dot(X_cube, fit_cube)) ** 2)\n",
    "train_rss_silly = np.mean((y - np.dot(X_silly, fit_silly)) ** 2)\n",
    "\n",
    "print(\"Trained RSS (quad): \", train_rss_quad)\n",
    "print(\"Trained RSS (cube): \", train_rss_cube)\n",
    "print(\"Trained RSS (silly): \", train_rss_silly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the crazy-high degree model does better at matching our training data. But we can visually see that this model has far too many parameters for the amount of data we have. It is able to fit the data while doing the wrong thing in general, as we can see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test set\n",
    "noise = np.sqrt(var) * np.random.randn(Xdom.shape[0])\n",
    "y_test = np.dot(Xdom[:, 0:3], [c, b, a]) + noise\n",
    "\n",
    "# Compute test set RSS for different models\n",
    "test_rss_quad = np.mean((y_test - np.dot(Xdom[:, 0:3], fit_quad)) ** 2)\n",
    "test_rss_cube = np.mean((y_test - np.dot(Xdom[:, 0:4], fit_cube)) ** 2)\n",
    "test_rss_silly = np.mean((y_test - np.dot(Xdom, fit_silly)) ** 2)\n",
    "\n",
    "print(\"Test RSS (quad): %0.5g\" % test_rss_quad)\n",
    "print(\"Test RSS (cube): %0.5g\" % test_rss_cube)\n",
    "print(\"Test RSS (silly): %0.5g\" % test_rss_silly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a huge error when tested over a larger range of possible values! This shows that we, in some way, need to make sure that we use the simplest model possible that generalizes. However, in practice, we often don't know how simple or complex the underlying data is. There are a couple of approaches here. One is to simply acquire more data, and make sure that you acquire data over the whole range of possible values. But, scaling up data acquisition might not be feasible for some applications. How can we tune the parameters that we use? Lets take a look at **regularization**. \n",
    "\n",
    "## Regularization\n",
    "In the examples we have looked at, we had a simple univariate problem and we were interested in making regressions according to functions (polynomials) of that cure datum. In this case, we have some discretion in choosing how complicated of a model to pick. And we have a natural way to scale the generalization capability of the model.\n",
    "\n",
    "However, in practice we might not have such a convenience. Often in regression tasks we have some response variable, perhaps a patient diagnosis or an experimental result, and we have many, many features which were measured in parallel with that observation. We would like to estabilish some model of the result given some combination of these measured features. \n",
    "\n",
    "In this case, *a priori*, we have no idea which possible features we measure will be the best predictors. Our idea is simply to acquire as much data as we can and write an algorithm to find the correlations in the dataset. In this case, we need to have some method of *selecting* the best possible features. Additionally, we are often taking measurements of different features which might be highly correlated, thus reducing the overall predictiveness of our regression model (with respect to the number of measured features).\n",
    "\n",
    "Regularization offers us a possiblity of accounting for some of these obstacles. We want to admit as complicated of a model as possible (to drive down fitting error), but we want some degree of regularization in order to also promote the *simplest* model as possible. Lets take a look at a few approaches.\n",
    "\n",
    "### Ridge Regression\n",
    "In ridge regression (RR), we want to fit our response variables, but we want the model parameters $\\boldsymbol{w}$ to have a small variance. We can accomplish this via an $\\ell_2$ penalty on the parameters,\n",
    "$$ \\hat{\\boldsymbol{w}} = {\\rm arg~min}_{\\boldsymbol{w}}\\quad\n",
    "    ||\\mathbf{y}-\\mathbf{X}\\boldsymbol{w}||_2^2 + \\lambda || \\boldsymbol{w}||_2^2,$$\n",
    "In this regression, the regularization term $\\lambda$ serves as a tuning term which balances between the model fit and the variance of the model parameters. Because this regularization penalizes \"long\" parameter vectors via the $\\ell_2$ norm, this kind of regression is often referred to as a **shrinkage**. \n",
    "\n",
    "There is a direct analytical solution to this regression, as in the case of least squares. We just need to solve\n",
    "$$ \\hat{\\boldsymbol{w}} = \n",
    "    (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "As a test case, lets look at a random problem from high-dimensional statistics. Often we are confronted with *many* features, but *few* measurements. However, we also often know that of the many features we measure, potentially very few of them contribute significantly to our response variables.\n",
    "\n",
    "In general, we cannot use an OLS estimator to solve such under-determined problems. There exists an entire null-space of potential solutions, and we have no information to guide us in how to make a unique solution. However, by using regression, we are enforcing a scoring to this space of solutions, allowing us to choose a \"best\" solution, up to how we define the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "n = 50           # Number of samples\n",
    "p = 100          # Number of parameters\n",
    "var = 0.1        # Model error variance\n",
    "lam = 0.1        # Regularization level\n",
    "\n",
    "# Generate features at random\n",
    "X = np.random.randn(n, p) / np.sqrt(p)\n",
    "\n",
    "# Create ground-truth model parameters\n",
    "w_model = np.zeros(p)\n",
    "w_model[15] = 3\n",
    "w_model[30] = -6\n",
    "\n",
    "# Form observations\n",
    "y = np.dot(X, w_model) + np.sqrt(var) * np.random.randn(n)\n",
    "\n",
    "# Ridge regression solution\n",
    "lam_eye = lam * np.eye(p)\n",
    "w_ridge = np.linalg.solve(np.dot(X.T, X) + lam_eye, np.dot(X.T, y))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.stem(w_ridge, label=r'Ridge $\\hat{w}$')\n",
    "plt.plot(np.argwhere(w_model), w_model[w_model != 0], 'kx', label=r'True $w$')\n",
    "plt.axis([0, p - 1, -7, 7])\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Coefficient value', fontsize=16)\n",
    "plt.legend(loc=4, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that voila, we have a solution! It isn't so close, but we can see that indeed we are able to see some correlation with the true model features. A question here is, what happens as we move the value of $\\lambda$. One common way of looking at the effect of the regularization term on the estimator is through the **L-Curve**. In this plot we look at the balance between the solution norm and the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty arrays\n",
    "lam_dom = np.logspace(-5, 2, 100)\n",
    "\n",
    "rss = np.zeros(len(lam_dom))\n",
    "reg = np.zeros(len(lam_dom))\n",
    "mse = np.zeros(len(lam_dom))\n",
    "d1 = np.zeros(len(lam_dom))\n",
    "d2 = np.zeros(len(lam_dom))\n",
    "\n",
    "# Compute different metrics for many $\\lambda$\n",
    "for (i, lam) in enumerate(lam_dom):\n",
    "    lam_eye = lam * np.eye(p)\n",
    "    fit = np.linalg.solve(np.dot(X.T,X) + lam_eye, np.dot(X.T,y))\n",
    "    \n",
    "    rss[i] = np.mean((y - np.dot(X, fit)) ** 2)\n",
    "    reg[i] = np.sum(fit ** 2)\n",
    "    mse[i] = np.mean((w_model - fit) ** 2)\n",
    "    d1[i] = fit[15]\n",
    "    d2[i] = fit[30]\n",
    "    \n",
    "best_lambda = lam_dom[np.argmin(mse)] \n",
    "best_rss = rss[np.argmin(mse)]\n",
    "best_reg = reg[np.argmin(mse)]\n",
    "    \n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 5))  \n",
    "\n",
    "plt.subplot(131)\n",
    "plt.loglog(rss, reg)\n",
    "plt.plot(rss[50], reg[50], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[50])\n",
    "plt.plot(rss[60], reg[60], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[60])\n",
    "plt.plot(rss[70], reg[70], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[70])\n",
    "plt.plot(rss[80], reg[80], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[80])\n",
    "plt.plot(best_rss, best_reg, '*k', label='$\\\\lambda^*$ = %0.1e' % best_lambda, markersize=10)\n",
    "plt.xlabel('Residual norm $||X w - y||_2^2$', fontsize=16)\n",
    "plt.ylabel('Solution norm $||w||_2^2$', fontsize=16)\n",
    "plt.legend(loc=3, fontsize=16)\n",
    "plt.title('L-Curve', fontsize=18)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.loglog(lam_dom, mse)\n",
    "plt.axvline(best_lambda, color='k', linestyle=':', label=r'$\\lambda^*$')\n",
    "plt.xlabel(r'Regularization term $\\lambda$', fontsize=16)\n",
    "plt.ylabel(r'MSE($w$, $\\hat{w}$)', fontsize=16)\n",
    "plt.title(r'True MSE vs. Regularization', fontsize=18)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(d1, d2)\n",
    "plt.plot(d1[0], d2[0],'ok', label='$\\\\lambda \\\\rightarrow 0$')\n",
    "plt.plot(d1[-1], d2[-1],'or', label='$\\\\lambda \\\\rightarrow \\\\infty$')\n",
    "plt.xlabel(r'$\\hat{w}_{15}$', fontsize=16)\n",
    "plt.ylabel(r'$\\hat{w}_{30}$', fontsize=16)\n",
    "plt.title('Coefficient path', fontsize=18)\n",
    "plt.legend(loc=1, fontsize=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the L-Curve is to find the \"balance\" point between minimizing the solution norm and minimizing the residual norm, i.e. the bend of the curve. Another approach is simply to use cross validation, but it gives you an idea of the effect of the regularization. As $\\lambda$ increases, the $||w||_2 \\rightarrow 0$, hence the term shrinkage. But, we can decrease our bias by going in the direction $\\lambda \\rightarrow 0$. Here, we see that the solution norm rises, but one can minimize the residual error. However, in this case, we will probably fail on generalization.\n",
    "\n",
    "Remarks: \n",
    "\n",
    "$\\bullet$ In the limit $\\lambda \\to 0$, we recover the Least Square (LS) solution of the non-regularized problem (cf.lectures) \n",
    "\n",
    "$\\bullet$ When we increase $\\lambda$, all the coefficients of $\\boldsymbol{w}$ shrink. However, if we are lucky, the ones that are just due to noise will shrink **faster** than the ones that are non-zero in the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "Another approach that we can use is the Lasso for sparse regression. In this case, rather than penalizing the energy of the coefficients, $||\\boldsymbol{w}||_2^2$, instead we penalize the $\\ell_1$ norm, \n",
    "$$||\\boldsymbol{w}||_1 = \\sum_i |w_i|.$$\n",
    "When we use this prior we are promoting the **sparsity** of the parameters. This should place hard-zeros on \"insignificant\" features, and assign most of the model weight to a few features. \n",
    "\n",
    "The estimator for the Lasso is written similarily as\n",
    "$$\\hat{\\boldsymbol{w}} = {\\rm arg~min}_{\\boldsymbol{w}} \n",
    "\\quad \\frac{1}{2N} ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{w} ||_2^2 + \\lambda ||\\boldsymbol{w}||_1 $$\n",
    "Remark: the factor $1/(2N)$ in front is a convention, that amounts to an overall scale for $\\lambda$\n",
    "\n",
    "How do we attempt to find this estimator? Unfortunately, there isn't a closed form solution for this estimator. Instead we need to write some convex optimization (or fixed-point iteration) to recover the estimator. We will do it at the end. For the moment, we will use the Lasso implemented in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize (in the sklearn routine, lambda is called alpha)\n",
    "lasso = linear_model.Lasso(alpha=0.01)\n",
    "\n",
    "# Fit to the data\n",
    "lasso.fit(X,y)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.stem(lasso.coef_, label=r'Lasso $\\hat{w}$')\n",
    "plt.plot(np.argwhere(w_model), w_model[w_model != 0], 'kx',label=r'True $w$')\n",
    "plt.axis([0, p-1, -7, 7])\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Coefficient value', fontsize=16)\n",
    "plt.legend(loc=4, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, we can now see that we can recover these two significant features quite easily using the $\\ell_1$-regularization ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty arrays\n",
    "lam_dom = np.logspace(-5, 2, 100)\n",
    "\n",
    "rss = np.zeros(len(lam_dom))\n",
    "reg = np.zeros(len(lam_dom))\n",
    "mse = np.zeros(len(lam_dom))\n",
    "d1 = np.zeros(len(lam_dom))\n",
    "d2 = np.zeros(len(lam_dom))\n",
    "\n",
    "# Compute different metrics for many $\\lambda$\n",
    "for (i, lam) in enumerate(lam_dom):\n",
    "    fit = linear_model.Lasso(alpha=lam * n).fit(X, y)\n",
    "    \n",
    "    rss[i] = np.mean((y - fit.predict(X)) ** 2)\n",
    "    reg[i] = np.sum(np.abs(fit.coef_))\n",
    "    mse[i] = np.mean((w_model - fit.coef_) ** 2)\n",
    "    d1[i] = fit.coef_[15]\n",
    "    d2[i] = fit.coef_[30]\n",
    "    \n",
    "best_lambda = lam_dom[np.argmin(mse)] \n",
    "best_rss = rss[np.argmin(mse)]\n",
    "best_reg = reg[np.argmin(mse)]\n",
    "    \n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 5))  \n",
    "\n",
    "plt.subplot(131)\n",
    "plt.loglog(rss, reg)\n",
    "plt.plot(rss[50], reg[50], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[50])\n",
    "plt.plot(rss[60], reg[60], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[60])\n",
    "plt.plot(rss[70], reg[70], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[70])\n",
    "plt.plot(rss[80], reg[80], 'o', label='$\\\\lambda = %0.1e$' % lam_dom[80])\n",
    "plt.plot(best_rss, best_reg, '*k', label='$\\\\lambda^*$ = %0.1e' % best_lambda, markersize=10)\n",
    "plt.xlabel('Residual norm $||X w - y||_2^2$', fontsize=16)\n",
    "plt.ylabel('Solution norm $||w||_2^2$', fontsize=16)\n",
    "plt.legend(loc=3, fontsize=16)\n",
    "plt.title('L-Curve', fontsize=18)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.loglog(lam_dom, mse)\n",
    "plt.axvline(best_lambda, color='k', linestyle=':', label=r'$\\lambda^*$')\n",
    "plt.xlabel(r'Regularization term $\\lambda$', fontsize=16)\n",
    "plt.ylabel(r'MSE($w$, $\\hat{w}$)', fontsize=16)\n",
    "plt.title(r'True MSE vs. Regularization', fontsize=18)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(d1, d2)\n",
    "plt.plot(d1[0], d2[0],'ok', label='$\\\\lambda \\\\rightarrow 0$')\n",
    "plt.plot(d1[-1], d2[-1],'or', label='$\\\\lambda \\\\rightarrow \\\\infty$')\n",
    "plt.xlabel(r'$\\hat{w}_{15}$', fontsize=16)\n",
    "plt.ylabel(r'$\\hat{w}_{30}$', fontsize=16)\n",
    "plt.title('Coefficient path', fontsize=18)\n",
    "plt.legend(loc=1, fontsize=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "\n",
    "$\\bullet$ For Lasso, one can prove that the coefficients all evolve linearly in $\\lambda$ until one of them vanishes; at that point, all the others evolve linearly but with different slopes.\n",
    "\n",
    "$\\bullet$ We can see in the 'coefficient path' plot that the last two coefficients to vanish when $\\lambda\\to\\infty$ are $\\hat{w}_{15}$ and then $\\hat{w}_{30}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso algorithm\n",
    "\n",
    "The estimator for the Lasso is\n",
    "$$\\hat{\\boldsymbol{w}} = {\\rm arg~min}_{\\boldsymbol{w}} \n",
    "\\quad \\frac1{2N} ||\\mathbf{y} - \\mathbf{X}\\boldsymbol{w} ||_2^2 + \\lambda ||\\boldsymbol{w}||_1 $$\n",
    "which is equivalent to\n",
    "$$\\hat{\\boldsymbol{w}} = {\\rm arg~min}_{\\boldsymbol{w}} \n",
    "\\quad  \\frac1{2N} (\\boldsymbol{w}^T X^T X \\boldsymbol{w} -2\\mathbf{y}^T\\mathbf{X}\\boldsymbol{w}) + \\lambda ||\\boldsymbol{w}||_1 $$\n",
    "\n",
    "Defining $A = X^T X/N$ and $B^T = \\mathbf{y}^T X/N$, we have\n",
    "$$\\hat{\\boldsymbol{w}} = {\\rm arg~min}_{\\boldsymbol{w}} \n",
    "\\quad  \\frac12\\boldsymbol{w}^T A \\boldsymbol{w} - B^T \\boldsymbol{w}  + \\lambda ||\\boldsymbol{w}||_1 $$\n",
    "\n",
    "Taking the derivative with respect to $w_i$, we obtain\n",
    "$$A_{ii} w_i + \\sum_{j(\\neq i)}A_{ij} w_j - B_i + \\lambda \\, \\operatorname{sign} (w_i) = A_{ii} w_i - V_i + \\lambda \\operatorname{sign} (w_i) = 0 $$\n",
    "where\n",
    "$$V_i = B_i - \\sum_{j(\\neq i)}A_{ij} w_j$$\n",
    "\n",
    "The solution is, if $|V_i| > \\lambda$\n",
    "$$w_i = (V_i - \\lambda \\operatorname{sign} (V_i)) \\, \\big/ \\, A_{ii}$$\n",
    "and zero otherwise.\n",
    "\n",
    "The Lasso algorithm consists in solving iteratively for each of the $w_i$ and repeat until convergence. Let us implement this scheme below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds minimum of 1 / (2n) * (y - Xw)^2 + \\gamma |w|, n = len(y)\n",
    "def my_lasso(X, y, gamma, eps=1e-5):\n",
    "    # Initialize variables\n",
    "    n, p = X.shape\n",
    "    w = np.zeros(p)\n",
    "\n",
    "    # Rephrase problem as .5 * w^T A w - B w + \\gamma |w|\n",
    "    A = np.dot(X.T, X) / n\n",
    "    B = np.dot(X.T, y) / n\n",
    "\n",
    "    # Iteration over all variables\n",
    "    diff = eps + 1\n",
    "    while (diff > eps):\n",
    "        w_old = np.copy(w)  \n",
    "        for j in range(p):\n",
    "            w[j] = 0\n",
    "            V = B[j] - np.dot(w, A[j, :])\n",
    "            if (abs(V) > gamma):          \n",
    "                w[j] = (V - gamma * np.sign(V)) / A[j, j]\n",
    "        diff = max(abs(w - w_old));\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate some very simple test data and compare our implementation to that of scikitlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Generate test data\n",
    "X = np.array([[1, 2, 3], [4, 3, 2]])\n",
    "w = np.array([1, 0, 0])\n",
    "y = np.dot(X, w)\n",
    "alpha = 1.\n",
    "\n",
    "# Run my_lasso\n",
    "%time w_hat = my_lasso(X, y, alpha)\n",
    "objective = sum((y - np.dot(X, w_hat)) ** 2) / (2 * len(y)) + alpha * sum(w_hat)\n",
    "print('my_lasso -- objective = %g, non-zero coefs.: %s\\n' % (objective, w_hat[[w_hat != 0]]))\n",
    "\n",
    "# Run scikitlearn's Lasso\n",
    "clf = linear_model.Lasso(alpha=malpha, fit_intercept=False)\n",
    "%time clf.fit(X, y) \n",
    "objective = sum((y - np.dot(X, clf.coef_)) ** 2) / (2 * len(y)) + alpha * sum(clf.coef_)\n",
    "print('sklearn\\'s Lasso -- objective = %g, non-zero coefs.: %s' % (objective, clf.coef_[[clf.coef_ != 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation actually ran faster than scikitlearn's, even though we did it in Python and theirs is in C! That is because our test data is very small, and just calling the C function takes longer than running it. Let us try again with a bigger instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "X = np.random.randn(20, 100)\n",
    "w = np.zeros(100)\n",
    "w[5] = 3.\n",
    "w[25] = -3.\n",
    "y = np.dot(X, w)\n",
    "alpha = 1.\n",
    "\n",
    "# Run my_lasso\n",
    "%time w_hat = my_lasso(X, y, alpha)\n",
    "objective = sum((y - np.dot(X, w_hat)) ** 2) / (2 * len(y)) + alpha * sum(w_hat)\n",
    "print('my_lasso -- objective = %g, non-zero coefs.: %s\\n' % (objective, w_hat[[w_hat != 0]]))\n",
    "\n",
    "# Run scikitlearn's Lasso\n",
    "clf = linear_model.Lasso(alpha=alpha, fit_intercept=False)\n",
    "%time clf.fit(X, y) \n",
    "objective = sum((y - np.dot(X, clf.coef_)) ** 2) / (2 * len(y)) + alpha * sum(clf.coef_)\n",
    "print('sklearn\\'s Lasso -- objective = %g, non-zero coefs.: %s' % (objective, clf.coef_[[clf.coef_ != 0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

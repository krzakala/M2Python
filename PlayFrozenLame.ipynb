{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning approach to the Frowen lake game#\n",
    "## A simple illustration of the q-learning algorithm ##\n",
    "We are going to teach a computer to play the Game \"FROZEN LAKE\" which, converniently enough, is provided by the gym environement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is the game is easy: one start at the top left of the screen, and needs to go down to the bottom right, avoiding the holes:\n",
    "![](Frozen-Lake.png\t)\n",
    "The player must deceide at each steps what he does, but beware, the lake is slipery, so even if you move left, from time to time you will find yourself moving in a different direction.\n",
    "\n",
    "As can be checked easyly, there are $4$ possible actions, and $16$ possible states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 4, number of possible states :16.\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print(\"Number of possible actions: %d, number of possible states :%d.\" % (action_size,state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to construct the Q-table that will give us, for each action, the total sum of reward in the future. In other words, the ideal table should be\n",
    "$$\n",
    "Q^*(s,a)=R^0(a)+\\sum_{t=1}^{\\infty} \\gamma^t R^t\n",
    "$$\n",
    "where $R^0$ is the immediate reward if action $a$ is taken, and $R^t$ is the best possible reward for all next possible times.\n",
    "\n",
    "Since we do not know this table *a priori* we start with a random guess Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.20126535e-05 9.51917993e-05 7.58549581e-06 4.20522717e-05]\n",
      " [5.93560679e-05 2.05266150e-05 7.73609902e-05 3.32627626e-05]\n",
      " [7.15391994e-05 7.81981546e-05 5.41331483e-06 1.38914220e-05]\n",
      " [8.88068036e-05 1.02153289e-05 4.83677338e-05 5.71423813e-05]\n",
      " [1.04484668e-06 6.92089489e-05 4.78812433e-05 6.40257544e-05]\n",
      " [9.59348388e-05 1.57587732e-06 5.01156269e-05 6.72649751e-05]\n",
      " [7.07425060e-05 1.06172168e-05 2.96746011e-05 4.19711066e-05]\n",
      " [9.82190629e-05 2.72426338e-05 7.39274089e-05 6.66954648e-05]\n",
      " [9.95079939e-05 2.24644631e-05 7.77881135e-05 8.00153735e-05]\n",
      " [3.46003198e-05 3.06226874e-05 7.38690638e-05 8.63406604e-05]\n",
      " [7.28911027e-05 7.16976933e-05 5.95028190e-05 4.41792308e-05]\n",
      " [3.26390314e-05 4.93716965e-05 1.73102804e-06 1.68723502e-05]\n",
      " [2.54424347e-05 5.87725510e-05 7.56597495e-05 9.60635491e-05]\n",
      " [3.13446334e-05 6.32912262e-05 7.90374495e-05 6.88290966e-05]\n",
      " [1.90775409e-05 8.01124821e-05 2.29613871e-05 9.05698301e-05]\n",
      " [8.67596055e-05 5.94842029e-05 9.12279735e-05 5.22916231e-06]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.random.uniform(0,1e-4,(state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn this table (the Q-learning part) we need to derive the Bellman equation. It follows from the remark that for the ideal table one has:\n",
    "$$Q^*(s,a)=R^0(a)+\\sum_{t=1}^{\\infty} \\gamma^t R^t= R^0(a) + \\gamma \\sum_{t=1}^{\\infty} \\gamma^{t-1} R^t = R^0(a) + \\gamma \\left[R^1  + \\sum_{\\tau=1}^{\\infty} \\gamma^{\\tau} R^{1+\\tau}\\right]$$\n",
    "\n",
    "Since $Q^*(s,a)$ is time-translation invariant, we can thus write:\n",
    "$$Q^*(s,a)= R^0(a) + \\gamma \\left[R_{\\rm best}^1  + \\sum_{\\tau=0}^{\\infty} \\gamma^{\\tau} R^{1+\\tau}\\right]$$\n",
    "and this leads to the **Bellman equation**:\n",
    "$$Q^*(s,a)=  R^0(a) + \\gamma \\max_{a'} Q^*(s'|a,a')$$\n",
    "\n",
    "Given this identity, we will use the update rule:\n",
    "$$\n",
    "Q^{t+1}(s,a)=(1-\\delta) * Q^{t}(s,a)+ \\delta(R(a) * \\gamma \\max_{a'} Q^*(s'|a,a')$$\n",
    "where $\\delta$ is the learning rate.\n",
    "\n",
    "The important point, before updating this table, is to set up an equilibrium between exploration and explotation when we play the game: of course, we want ultimatly to play the game according to the q-table $Q^*$ (*exploitation*) but since, at the begiging, our table $Q$ is essentially random, we should also try from to time to time to allow random moves (exploration). \n",
    "\n",
    "We shall do this less and less over time, of course, and every time we play a new game (a new episode) we should start to beleive more and more our table and so we shall set the eploration rate as\n",
    "$$\n",
    "\\epsilon^t = \\epsilon_{\\min} + (\\epsilon_{\\max} - \\epsilon_{\\min})e^{-n_{\\rm episode} \\lambda} \n",
    "$$\n",
    "where $\\lambda$ is a decay rate.\n",
    "\n",
    "Let us set up all these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 20000      # Total episodes (number of games played)\n",
    "learning_rate = 0.5         # Learning rate in Bellman equation (delta)\n",
    "max_steps = 99              # Max steps per episode\n",
    "gamma = 0.99                # Discounting rate in the Q-table\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to write the learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game numer 0: total reward:0.000000\n",
      "Game numer 100: total reward:0.000000\n",
      "Game numer 200: total reward:0.020000\n",
      "Game numer 300: total reward:0.020000\n",
      "Game numer 400: total reward:0.000000\n",
      "Game numer 500: total reward:0.020000\n",
      "Game numer 600: total reward:0.040000\n",
      "Game numer 700: total reward:0.030000\n",
      "Game numer 800: total reward:0.100000\n",
      "Game numer 900: total reward:0.090000\n",
      "Game numer 1000: total reward:0.070000\n",
      "Game numer 1100: total reward:0.080000\n",
      "Game numer 1200: total reward:0.080000\n",
      "Game numer 1300: total reward:0.060000\n",
      "Game numer 1400: total reward:0.120000\n",
      "Game numer 1500: total reward:0.150000\n",
      "Game numer 1600: total reward:0.170000\n",
      "Game numer 1700: total reward:0.240000\n",
      "Game numer 1800: total reward:0.290000\n",
      "Game numer 1900: total reward:0.120000\n",
      "Game numer 2000: total reward:0.250000\n",
      "Game numer 2100: total reward:0.240000\n",
      "Game numer 2200: total reward:0.230000\n",
      "Game numer 2300: total reward:0.210000\n",
      "Game numer 2400: total reward:0.320000\n",
      "Game numer 2500: total reward:0.250000\n",
      "Game numer 2600: total reward:0.390000\n",
      "Game numer 2700: total reward:0.260000\n",
      "Game numer 2800: total reward:0.420000\n",
      "Game numer 2900: total reward:0.280000\n",
      "Game numer 3000: total reward:0.400000\n",
      "Game numer 3100: total reward:0.330000\n",
      "Game numer 3200: total reward:0.300000\n",
      "Game numer 3300: total reward:0.380000\n",
      "Game numer 3400: total reward:0.270000\n",
      "Game numer 3500: total reward:0.390000\n",
      "Game numer 3600: total reward:0.450000\n",
      "Game numer 3700: total reward:0.320000\n",
      "Game numer 3800: total reward:0.540000\n",
      "Game numer 3900: total reward:0.440000\n",
      "Game numer 4000: total reward:0.600000\n",
      "Game numer 4100: total reward:0.450000\n",
      "Game numer 4200: total reward:0.390000\n",
      "Game numer 4300: total reward:0.400000\n",
      "Game numer 4400: total reward:0.430000\n",
      "Game numer 4500: total reward:0.570000\n",
      "Game numer 4600: total reward:0.590000\n",
      "Game numer 4700: total reward:0.610000\n",
      "Game numer 4800: total reward:0.490000\n",
      "Game numer 4900: total reward:0.640000\n",
      "Game numer 5000: total reward:0.700000\n",
      "Game numer 5100: total reward:0.570000\n",
      "Game numer 5200: total reward:0.590000\n",
      "Game numer 5300: total reward:0.610000\n",
      "Game numer 5400: total reward:0.490000\n",
      "Game numer 5500: total reward:0.430000\n",
      "Game numer 5600: total reward:0.600000\n",
      "Game numer 5700: total reward:0.460000\n",
      "Game numer 5800: total reward:0.590000\n",
      "Game numer 5900: total reward:0.520000\n",
      "Game numer 6000: total reward:0.600000\n",
      "Game numer 6100: total reward:0.560000\n",
      "Game numer 6200: total reward:0.530000\n",
      "Game numer 6300: total reward:0.630000\n",
      "Game numer 6400: total reward:0.340000\n",
      "Game numer 6500: total reward:0.530000\n",
      "Game numer 6600: total reward:0.560000\n",
      "Game numer 6700: total reward:0.400000\n",
      "Game numer 6800: total reward:0.490000\n",
      "Game numer 6900: total reward:0.630000\n",
      "Game numer 7000: total reward:0.450000\n",
      "Game numer 7100: total reward:0.620000\n",
      "Game numer 7200: total reward:0.680000\n",
      "Game numer 7300: total reward:0.650000\n",
      "Game numer 7400: total reward:0.460000\n",
      "Game numer 7500: total reward:0.570000\n",
      "Game numer 7600: total reward:0.470000\n",
      "Game numer 7700: total reward:0.560000\n",
      "Game numer 7800: total reward:0.710000\n",
      "Game numer 7900: total reward:0.660000\n",
      "Game numer 8000: total reward:0.610000\n",
      "Game numer 8100: total reward:0.570000\n",
      "Game numer 8200: total reward:0.680000\n",
      "Game numer 8300: total reward:0.590000\n",
      "Game numer 8400: total reward:0.570000\n",
      "Game numer 8500: total reward:0.570000\n",
      "Game numer 8600: total reward:0.580000\n",
      "Game numer 8700: total reward:0.600000\n",
      "Game numer 8800: total reward:0.570000\n",
      "Game numer 8900: total reward:0.560000\n",
      "Game numer 9000: total reward:0.660000\n",
      "Game numer 9100: total reward:0.600000\n",
      "Game numer 9200: total reward:0.560000\n",
      "Game numer 9300: total reward:0.590000\n",
      "Game numer 9400: total reward:0.490000\n",
      "Game numer 9500: total reward:0.540000\n",
      "Game numer 9600: total reward:0.650000\n",
      "Game numer 9700: total reward:0.610000\n",
      "Game numer 9800: total reward:0.550000\n",
      "Game numer 9900: total reward:0.590000\n",
      "Game numer 10000: total reward:0.650000\n",
      "Game numer 10100: total reward:0.570000\n",
      "Game numer 10200: total reward:0.390000\n",
      "Game numer 10300: total reward:0.620000\n",
      "Game numer 10400: total reward:0.720000\n",
      "Game numer 10500: total reward:0.580000\n",
      "Game numer 10600: total reward:0.590000\n",
      "Game numer 10700: total reward:0.630000\n",
      "Game numer 10800: total reward:0.530000\n",
      "Game numer 10900: total reward:0.570000\n",
      "Game numer 11000: total reward:0.600000\n",
      "Game numer 11100: total reward:0.640000\n",
      "Game numer 11200: total reward:0.560000\n",
      "Game numer 11300: total reward:0.820000\n",
      "Game numer 11400: total reward:0.610000\n",
      "Game numer 11500: total reward:0.660000\n",
      "Game numer 11600: total reward:0.700000\n",
      "Game numer 11700: total reward:0.590000\n",
      "Game numer 11800: total reward:0.410000\n",
      "Game numer 11900: total reward:0.630000\n",
      "Game numer 12000: total reward:0.580000\n",
      "Game numer 12100: total reward:0.660000\n",
      "Game numer 12200: total reward:0.640000\n",
      "Game numer 12300: total reward:0.620000\n",
      "Game numer 12400: total reward:0.590000\n",
      "Game numer 12500: total reward:0.660000\n",
      "Game numer 12600: total reward:0.610000\n",
      "Game numer 12700: total reward:0.560000\n",
      "Game numer 12800: total reward:0.650000\n",
      "Game numer 12900: total reward:0.560000\n",
      "Game numer 13000: total reward:0.640000\n",
      "Game numer 13100: total reward:0.480000\n",
      "Game numer 13200: total reward:0.520000\n",
      "Game numer 13300: total reward:0.580000\n",
      "Game numer 13400: total reward:0.660000\n",
      "Game numer 13500: total reward:0.650000\n",
      "Game numer 13600: total reward:0.490000\n",
      "Game numer 13700: total reward:0.580000\n",
      "Game numer 13800: total reward:0.590000\n",
      "Game numer 13900: total reward:0.680000\n",
      "Game numer 14000: total reward:0.610000\n",
      "Game numer 14100: total reward:0.500000\n",
      "Game numer 14200: total reward:0.640000\n",
      "Game numer 14300: total reward:0.640000\n",
      "Game numer 14400: total reward:0.650000\n",
      "Game numer 14500: total reward:0.540000\n",
      "Game numer 14600: total reward:0.620000\n",
      "Game numer 14700: total reward:0.660000\n",
      "Game numer 14800: total reward:0.690000\n",
      "Game numer 14900: total reward:0.610000\n",
      "Game numer 15000: total reward:0.530000\n",
      "Game numer 15100: total reward:0.620000\n",
      "Game numer 15200: total reward:0.640000\n",
      "Game numer 15300: total reward:0.400000\n",
      "Game numer 15400: total reward:0.560000\n",
      "Game numer 15500: total reward:0.470000\n",
      "Game numer 15600: total reward:0.590000\n",
      "Game numer 15700: total reward:0.790000\n",
      "Game numer 15800: total reward:0.620000\n",
      "Game numer 15900: total reward:0.620000\n",
      "Game numer 16000: total reward:0.520000\n",
      "Game numer 16100: total reward:0.660000\n",
      "Game numer 16200: total reward:0.600000\n",
      "Game numer 16300: total reward:0.610000\n",
      "Game numer 16400: total reward:0.520000\n",
      "Game numer 16500: total reward:0.540000\n",
      "Game numer 16600: total reward:0.700000\n",
      "Game numer 16700: total reward:0.670000\n",
      "Game numer 16800: total reward:0.650000\n",
      "Game numer 16900: total reward:0.690000\n",
      "Game numer 17000: total reward:0.680000\n",
      "Game numer 17100: total reward:0.640000\n",
      "Game numer 17200: total reward:0.510000\n",
      "Game numer 17300: total reward:0.540000\n",
      "Game numer 17400: total reward:0.700000\n",
      "Game numer 17500: total reward:0.620000\n",
      "Game numer 17600: total reward:0.710000\n",
      "Game numer 17700: total reward:0.510000\n",
      "Game numer 17800: total reward:0.580000\n",
      "Game numer 17900: total reward:0.650000\n",
      "Game numer 18000: total reward:0.560000\n",
      "Game numer 18100: total reward:0.470000\n",
      "Game numer 18200: total reward:0.460000\n",
      "Game numer 18300: total reward:0.470000\n",
      "Game numer 18400: total reward:0.650000\n",
      "Game numer 18500: total reward:0.670000\n",
      "Game numer 18600: total reward:0.580000\n",
      "Game numer 18700: total reward:0.590000\n",
      "Game numer 18800: total reward:0.470000\n",
      "Game numer 18900: total reward:0.670000\n",
      "Game numer 19000: total reward:0.690000\n",
      "Game numer 19100: total reward:0.630000\n",
      "Game numer 19200: total reward:0.410000\n",
      "Game numer 19300: total reward:0.560000\n",
      "Game numer 19400: total reward:0.620000\n",
      "Game numer 19500: total reward:0.590000\n",
      "Game numer 19600: total reward:0.520000\n",
      "Game numer 19700: total reward:0.550000\n",
      "Game numer 19800: total reward:0.560000\n",
      "Game numer 19900: total reward:0.510000\n",
      "[[5.86065596e-01 3.99202442e-01 5.20776354e-01 3.89430729e-01]\n",
      " [1.58299283e-01 1.73252290e-01 2.21655701e-01 3.89548420e-01]\n",
      " [2.45317909e-01 2.35203264e-01 2.40879945e-01 3.75751560e-01]\n",
      " [3.40016244e-02 2.20272678e-01 2.32654356e-01 3.73637526e-01]\n",
      " [5.86334317e-01 1.16031215e-01 2.13333343e-01 8.90919624e-02]\n",
      " [9.59348388e-05 1.57587732e-06 5.01156269e-05 6.72649751e-05]\n",
      " [2.14255942e-02 5.50159864e-04 1.41297301e-02 9.81850999e-05]\n",
      " [9.82190629e-05 2.72426338e-05 7.39274089e-05 6.66954648e-05]\n",
      " [3.26794750e-01 4.14995131e-01 1.83485373e-01 6.11135915e-01]\n",
      " [2.62116295e-01 6.47618283e-01 1.54392855e-01 2.37173988e-01]\n",
      " [6.15871034e-01 4.76978424e-01 1.02591082e-01 1.31816619e-01]\n",
      " [3.26390314e-05 4.93716965e-05 1.73102804e-06 1.68723502e-05]\n",
      " [2.54424347e-05 5.87725510e-05 7.56597495e-05 9.60635491e-05]\n",
      " [2.78033270e-01 1.25083126e-01 7.63802781e-01 1.88657349e-01]\n",
      " [4.61837387e-01 9.28494532e-01 4.32830489e-01 6.36024319e-01]\n",
      " [8.67596055e-05 5.94842029e-05 9.12279735e-05 5.22916231e-06]]\n"
     ]
    }
   ],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# For each episode/game, we play:\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    #Now, we play until dead or until it became toooooooo long\n",
    "    for step in range(max_steps):\n",
    "        # First we deceide if we play in or out of policy:\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Now we take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Finally we perform the Bellman update...\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        #... and update the reward for this game.\n",
    "        # Note that here, we only get a reward 1 if we eventually reach the goal!\n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        #other we continue to play\n",
    "        \n",
    "    # Reduce epsilon after each game/episode\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    #update rewards\n",
    "    rewards.append(total_rewards)\n",
    "    \n",
    "    if(episode%100==0):\n",
    "        av_rewards=sum(rewards)/100;\n",
    "        print(\"Game numer %d: total reward:%f\" %(episode,av_rewards))\n",
    "        rewards = []\n",
    "\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now watch our little q-table playing the game, this time using *in policty* moves only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=0 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=1 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=2 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=3 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=4 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=5 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=6 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=7 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=8 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=9 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=10 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "t=11 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=12 \n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=13 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=14 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=15 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "t=16 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=17 \n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=18 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=19 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=20 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "t=21 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "t=22 \n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "t=23 \n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=24 \n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=25 \n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=26 \n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=27 \n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=28 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=29 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=30 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=31 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=32 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=33 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=34 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=35 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=36 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=37 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=38 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=39 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=40 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=41 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=42 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=43 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=44 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=45 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=46 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=47 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=48 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=49 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=50 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=51 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=52 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=53 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=54 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=55 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=56 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=57 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=58 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=59 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=60 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=61 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=62 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=63 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=64 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=65 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=66 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=67 \n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=68 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=69 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=70 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=71 \n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=72 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=73 \n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=74 \n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=75 \n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "t=76 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=77 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "t=78 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=79 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=80 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "t=81 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=82 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "t=83 \n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "t=84 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "t=85 \n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "t=86 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "t=87 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "t=88 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "t=89 \n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "t=90 \n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "t=91 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "t=92 \n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "t=93 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "t=94 \n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "t=95 \n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 95\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "for step in range(max_steps):\n",
    "    print (\"t=%d \" % (step))\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = np.argmax(qtable[state,:])\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.render()            \n",
    "        # We print the number of step it took.\n",
    "        print(\"Number of steps\", step)\n",
    "        break\n",
    "    #else we move    \n",
    "    state = new_state\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing the parameters, we should be able to make an algortihm quite capable to play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

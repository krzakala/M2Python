{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import dataset_info, dataset_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Experiments\n",
    "\n",
    "Now lets take a look at a particular real-life data problem. In the following example we take a look at the prediction of ozone concentration as a factor of other weather-based features. As with all data problems, it behooves us to take a look at all of the information that we have about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info('laozone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at what this dataset looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_load('laozone')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we're ready to get started ! Now, before we touch anything, we need to follow best practices. When faced with a new dataset, we need to set up some kind of objective comparison. To do this, we need to split our dataset into three parts: **Training**(and within that, **Validation**), and **Testing** sets. \n",
    "\n",
    "The best practice here is to take the test data and lock it away somewhere. It is always tempting to tune your algorithms to give the best test performance. However, even if the regression isn't explicitly *trained* on the test data, as practitioners, we could be continually making changes in an effort to get our numbers up.\n",
    "\n",
    "Instead, we should deep-freeze the test data, and then tune as much as we can via **cross-validation (CV)** on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Convert from DataFrame to array\n",
    "y = data['ozone'].as_matrix().astype(float)\n",
    "X = data[[i for i in range(1,10)]].as_matrix().astype(float)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=1)\n",
    "\n",
    "print(\"Training samples: \", len(y_train))\n",
    "print(\"Testing samples: \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we start attempting to fit models, lets take a bit of care and apply some pre-processing to our dataset. The de-facto pre-processing is *centering and normalization*. Specifically, many flavors of estimators (OLS, RR, etc.) can be thrown of by large differences in of scale and variations between the features. We can easily account for this in our estimators by simply normalizing the feature columns and removing averages. Scikit-Learn has some features for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Adding features\n",
    "# Can you think of any other possible features to include, here?\n",
    "# What other pre-processing steps might you use?\n",
    "\n",
    "# Center and scale features\n",
    "X_test = preprocessing.scale(X_test)\n",
    "X_train = preprocessing.scale(X_train)\n",
    "\n",
    "# Center observations\n",
    "mean_train = np.mean(y_train)\n",
    "mean_test = np.mean(y_test)\n",
    "\n",
    "y_train = y_train - mean_train\n",
    "y_test = y_test - mean_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for us to choose our estimator. What should we choose? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1: OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the OLS estimate\n",
    "reg_ols = np.linalg.solve(np.dot(X_train.T, X_train), np.dot(X_train.T, y_train))\n",
    "\n",
    "yp = np.dot(X_train, reg_ols)\n",
    "yp_test = np.dot(X_test, reg_ols)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(y_train + mean_train,yp + mean_train, '.', label='Training')\n",
    "plt.plot(y_test + mean_test, yp_test + mean_test, '.r', label='Testing', marker='x')\n",
    "plt.plot([0, 40], [0, 40], '-k', linewidth=0.7, label=\"Perfect\")\n",
    "plt.axis([0, 40, 0, 40])\n",
    "plt.xlabel('Ozone (true)', fontsize=16)\n",
    "plt.ylabel('Ozone (predicted)', fontsize=16)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Prediction performance', fontsize=18)\n",
    "\n",
    "# Plot the learned model\n",
    "plt.subplot(122)\n",
    "plt.stem(reg_ols)\n",
    "plt.title(r'Learned model $\\hat{w}$', fontsize=18)\n",
    "plt.xticks(range(9),data.keys()[1:10], rotation='vertical')\n",
    "\n",
    "# Print RSS\n",
    "rss_train = np.mean((y_train - yp) ** 2)\n",
    "rss_test = np.mean((y_test - yp_test) ** 2)\n",
    "print(\"Normalized RSS (train): %0.2f\" % rss_train)\n",
    "print(\"Normalized RSS (test): %0.2f\" % rss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Define regression estimator\n",
    "reg_ridge = linear_model.Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to perform some kind of CV to find the best set of parameters for our model. We will do this by constructing a **Pipeline**. A pipeline is a useful way of handling pre-processing on separate data partitions when performing CV. Let's take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "# Make a pre-processing + fitting pipeline\n",
    "pipeline_ridge = make_pipeline(preprocessing.StandardScaler(), reg_ridge)\n",
    "\n",
    "# Define a scoring metric\n",
    "# To compare fits, we look at the prediction error via the RSS.\n",
    "def neg_rss(reg, X, y):\n",
    "    yp = reg.predict(X)\n",
    "    return -np.mean((y - yp) ** 2)\n",
    "\n",
    "# Define CV splitting\n",
    "# We can create an iterator which performs a set of randomized \n",
    "# splits on the dataset into \"train\" and \"validation\". We have\n",
    "# a natural tradeoff between the test set size and the number of \n",
    "# splits we should perform\n",
    "cv = ShuffleSplit(n_splits=20, test_size=0.05, random_state=0)\n",
    "\n",
    "# Define parameters to search\n",
    "# We need to specify the estimator name since we are performing a CV on\n",
    "# a Pipeline. (e.g. the formatting of `<estimator>__<param>`).\n",
    "param_grid = [\n",
    "    {'ridge__alpha': np.logspace(-4, 3, 50)}\n",
    "]\n",
    "\n",
    "# Run the CV\n",
    "cv_ridge = GridSearchCV(pipeline_ridge, param_grid, scoring=neg_rss, cv=cv)\n",
    "cv_ridge.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! Now lets take a look at the performance of our estimator. Here, for an example of Ridge or Lasso regression, we chart over the $\\alpha$ parameter that we perform CV against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record CV optimizing hyper-params\n",
    "opt_alpha = cv_ridge.cv_results_['param_ridge__alpha'][cv_ridge.best_index_]\n",
    "opt_params = cv_ridge.cv_results_['params'][cv_ridge.best_index_]\n",
    "\n",
    "# Visualize\n",
    "tested_alpha = cv_ridge.cv_results_['param_ridge__alpha']\n",
    "train_scores = -cv_ridge.cv_results_['mean_train_score']   # Reverse sign\n",
    "test_scores = -cv_ridge.cv_results_['mean_test_score']     # Reverse sign\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(tested_alpha, train_scores, '-', label='Training (avg.)')\n",
    "plt.plot(tested_alpha, test_scores, '-', label='Validation (avg.)')\n",
    "plt.xlabel(r'Regularization parameter $\\alpha$', fontsize=16)\n",
    "plt.ylabel(r'$\\frac{1}{N} RSS(y - X w)$', fontsize=16)\n",
    "plt.axvline(opt_alpha, label=r'$\\alpha^*$', color='k', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.xlim([1e-4, 1e3])\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=2, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, finally, we are ready to take our test data out of deep-freeze. How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_test = pipeline_ridge.predict(X_test)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(y_train + mean_train, yp + mean_train, '.', label='Training (CV-opt)')\n",
    "plt.plot(y_test + mean_test, yp_test + mean_test, '.r', label='Testing (CV-opt)', marker='x')\n",
    "plt.plot([0, 40], [0, 40], '-k', linewidth=0.7, label=\"Perfect\")\n",
    "plt.axis([0, 40, 0, 40])\n",
    "plt.xlabel('Ozone (true)', fontsize=16)\n",
    "plt.ylabel('Ozone (predicted)', fontsize=16)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Prediction Performance', fontsize=18)\n",
    "\n",
    "# Plot the learned model\n",
    "plt.subplot(122)\n",
    "plt.stem(reg_ridge.coef_)\n",
    "plt.title(r'Learned Model $\\hat{w}$', fontsize=18)\n",
    "plt.xticks(range(9), data.keys()[1:10], rotation='vertical')\n",
    "\n",
    "# Print RSS\n",
    "print(\"Normalized RSS (Train): %0.2f\" % -neg_rss(pipeline_ridge, X_train, y_train))\n",
    "print(\"Normalized RSS  (Test): %0.2f\" % -neg_rss(pipeline_ridge, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3: Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pre-processing + fitting pipeline\n",
    "reg_lasso = linear_model.Lasso(alpha=1.0)\n",
    "pipeline_lasso = make_pipeline(preprocessing.StandardScaler(), reg_lasso)\n",
    "\n",
    "# Define a scoring metric\n",
    "# To compare fits, we look at the prediction error via the RSS.\n",
    "def neg_rss(reg, X, y):\n",
    "    yp = reg.predict(X)\n",
    "    return -np.mean((y - yp) ** 2)\n",
    "\n",
    "# Define CV splitting\n",
    "# We can create an iterator which performs a set of randomized \n",
    "# splits on the dataset into \"train\" and \"validation\". We have\n",
    "# a natural tradeoff between the test set size and the number of \n",
    "# splits we should perform\n",
    "cv = ShuffleSplit(n_splits=20, test_size=0.05, random_state=0)\n",
    "\n",
    "# Define parameters to search\n",
    "# We need to specify the estimator name since we are performing a CV on\n",
    "# a Pipeline. (e.g. the formatting of `<estimator>__<param>`).\n",
    "param_grid = [\n",
    "    {'lasso__alpha': np.logspace(-4, 3, 50)}\n",
    "]\n",
    "\n",
    "# Run the CV\n",
    "cv_lasso = GridSearchCV(pipeline_lasso, param_grid, scoring=neg_rss, cv=cv)\n",
    "cv_lasso.fit(X_train,y_train)\n",
    "\n",
    "# Record CV optimizing hyper-params\n",
    "opt_alpha = cv_lasso.cv_results_['param_lasso__alpha'][cv_lasso.best_index_]\n",
    "opt_params = cv_lasso.cv_results_['params'][cv_lasso.best_index_]\n",
    "\n",
    "# Visualize\n",
    "tested_alpha = cv_lasso.cv_results_['param_lasso__alpha']\n",
    "train_scores = -cv_lasso.cv_results_['mean_train_score']   # Reverse sign\n",
    "test_scores = -cv_lasso.cv_results_['mean_test_score']     # Reverse sign\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(tested_alpha, train_scores, '-', label='Training (avg.)')\n",
    "plt.plot(tested_alpha, test_scores, '-', label='Validation (avg.)')\n",
    "plt.xlabel(r'Regularization parameter $\\alpha$', fontsize=16)\n",
    "plt.ylabel(r'$\\frac{1}{N} RSS(y - X w)$', fontsize=16)\n",
    "plt.axvline(opt_alpha, label=r'$\\alpha^*$', color='k', linestyle=':')\n",
    "plt.xscale('log')\n",
    "plt.xlim([1e-4, 1e3])\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=2, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training predictions\n",
    "pipeline_lasso.set_params(lasso__alpha=opt_alpha)\n",
    "pipeline_lasso.fit(X_train,y_train)\n",
    "yp = pipeline_lasso.predict(X_train)\n",
    "yp_test = pipeline_lasso.predict(X_test)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(y_train + mean_train, yp + mean_train, '.', label='Training (CV-opt)')\n",
    "plt.plot(y_test + mean_test, yp_test + mean_test, '.r', label='Testing (CV-opt)', marker='x')\n",
    "plt.plot([0, 40], [0, 40], '-k', linewidth=0.7, label=\"Perfect\")\n",
    "plt.axis([0, 40, 0, 40])\n",
    "plt.xlabel('Ozone (true)', fontsize=16)\n",
    "plt.ylabel('Ozone (predicted)', fontsize=16)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Prediction Performance', fontsize=18)\n",
    "\n",
    "# Plot the learned model\n",
    "plt.subplot(122)\n",
    "plt.stem(reg_lasso.coef_)\n",
    "plt.title(r'Learned Model $\\hat{w}$', fontsize=18)\n",
    "plt.xticks(range(9), data.keys()[1:10], rotation='vertical')\n",
    "\n",
    "# Print RSS\n",
    "print(\"Normalized RSS (Train): %0.2f\" % -neg_rss(pipeline_lasso, X_train, y_train))\n",
    "print(\"Normalized RSS  (Test): %0.2f\" % -neg_rss(pipeline_lasso, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "With boosing methods, over-training becomes a very real possibility. In this case we need to make sure to use our CV in order to stop fine-tuning our boosting approach when we start to have a loss on the validation data.\n",
    "\n",
    "We also have many possible different metrics to use in this case, not just the RSS. So, it is important to use our CV parameter grid to check many possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# Create estimator object\n",
    "reg_gb = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "# Make a pre-processing + fitting pipeline\n",
    "pipeline_gb = make_pipeline(preprocessing.StandardScaler(), reg_gb)\n",
    "\n",
    "# Define the parameters to search\n",
    "# We need to specify the estimator name since we are performing a CV on\n",
    "# a Pipeline. (e.g. the formatting of `<estimator>__<param>`).\n",
    "param_grid = [\n",
    "    {'gradientboostingregressor__loss': ['ls','lad'], \n",
    "     'gradientboostingregressor__learning_rate': np.logspace(-3,0,10),\n",
    "     'gradientboostingregressor__n_estimators': range(50,200,50)} \n",
    "]\n",
    "\n",
    "# Run the CV\n",
    "cv_gb = GridSearchCV(pipeline_gb, param_grid, scoring=neg_rss, cv=cv)\n",
    "cv_gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "cv_gb.cv_results_['params'][cv_gb.best_index_]\n",
    "cv_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training predictions\n",
    "cvopt_gb = cv_gb.best_estimator_\n",
    "cvopt_gb.fit(X_train,y_train)\n",
    "yp = cvopt_gb.predict(X_train)\n",
    "yp_test = cvopt_gb.predict(X_test)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot prediction performance\n",
    "plt.subplot(121)\n",
    "plt.plot(y_train + mean_train, yp + mean_train, '.', label='Training (CV-opt)')\n",
    "plt.plot(y_test + mean_test, yp_test + mean_test, '.r', label='Testing (CV-opt)', marker='x')\n",
    "plt.plot([0, 40], [0, 40], '-k', linewidth=0.7, label=\"Perfect\")\n",
    "plt.axis([0, 40, 0, 40])\n",
    "plt.xlabel('Ozone (true)', fontsize=16)\n",
    "plt.ylabel('Ozone (predicted)', fontsize=16)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Prediction Performance', fontsize=18)\n",
    "\n",
    "# Plot the learned model\n",
    "plt.subplot(122)\n",
    "plt.stem(cvopt_gb.named_steps['gradientboostingregressor'].feature_importances_)\n",
    "plt.title(r'Learned model $\\hat{w}$', fontsize=18)\n",
    "plt.ylabel('Feature importances', fontsize=16)\n",
    "plt.xticks(range(9),data.keys()[1:10], rotation='vertical')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print RSS\n",
    "print(\"Normalized RSS (train): %0.2f\" % -neg_rss(cvopt_gb, X_train, y_train))\n",
    "print(\"Normalized RSS (test): %0.2f\" % -neg_rss(cvopt_gb, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

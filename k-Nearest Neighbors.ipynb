{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first generate some synthetic data. Our *generative model* will be a Gaussian mixture model (GMM): for each class, we first sample 10 different centroids ${\\vec m}_k$ from $\\mathcal{N} ({\\vec m}_k; {\\vec 0}, I_2)$, then pick centroids at random and sample the actual data ${\\vec x}_i$ from $\\mathcal{N} ({\\vec x}_i; {\\vec m}_k, \\frac{1}{5} I_2)$.\n",
    "\n",
    "We start by sampling the centroids and plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples 10 centroids for each class from two different bivariate Normal distributions\n",
    "centroids_per_class = 10\n",
    "\n",
    "class0_centroids = [1, 0] + np.random.randn(centroids_per_class, 2)\n",
    "class1_centroids = [0, 1] + np.random.randn(centroids_per_class, 2)\n",
    "\n",
    "# Plot centroids\n",
    "plt.plot(class0_centroids[:, 0], class0_centroids[:, 1], \"o\", markersize=8)\n",
    "plt.plot(class1_centroids[:, 0], class1_centroids[:, 1], \"x\", markersize=8)\n",
    "plt.title(\"Centroids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the centroids we can sample the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class = 100\n",
    "\n",
    "# Sample actual data sampling from Normal distributions positioned around the centroids\n",
    "class0_labels = np.random.randint(10, size = samples_per_class)\n",
    "class1_labels = np.random.randint(10, size = samples_per_class)\n",
    "\n",
    "class0_samples = class0_centroids[class0_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
    "class1_samples = class1_centroids[class1_labels, :] + np.sqrt(1. / 5) * np.random.randn(samples_per_class, 2)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(class0_samples[:, 0], class0_samples[:, 1], \"o\", markersize=8)\n",
    "plt.plot(class1_samples[:, 0], class1_samples[:, 1], \"x\", markersize=8)\n",
    "plt.title(\"Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the problem we want to solve: we are given these points and we want to find a decision boundary that ensures generalization, i.e. that we are gonna be able to correctly classify new samples once they are given to us.\n",
    "\n",
    "Let us group the data in a nice way. For binary classification problems like this one, the way data is usually arranged is in\n",
    "\n",
    "- a feature matrix $X$ of size $N \\times P$, where $N$ is the number of samples and $P$ is the number of features (in our case $N = 200$ and $P = 2$);\n",
    "- a label vector $y \\in \\{0, 1\\}^N$ saying to which class each sample belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((class0_samples, class1_samples))\n",
    "y = np.hstack((np.zeros(samples_per_class), np.ones(samples_per_class)))\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "n_samples, n_features = np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the distance matrix, a $N \\times N$ matrix containing the distance from each sample to all others (do we really need to?).\n",
    "\n",
    "**Exercise**: compute the distance matrix. Extra points: do it in different ways and compare how long each one takes using the *magic command* `%timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(X):\n",
    "    n_samples = len(X)\n",
    "    distances = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Here goes the algorithm\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            distances[i, j] =\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see the solution\n",
    "# %load knn1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the distance matrix we can now write our algorithm!\n",
    "\n",
    "**Exercise**: write a function that compute the $k$-nearest neighbor estimate for each point in the training set. Tip: look up for the `np.argpartition` function. What is training error for $k = 10$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.argpartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X, y, k):\n",
    "    n_samples = len(y)\n",
    "    distances = cdist(X, X)\n",
    "    estimate = np.zeros(n_samples)\n",
    "    \n",
    "    # For each sample in the training set...\n",
    "    for i in range(n_samples):\n",
    "        # Look up k closest samples\n",
    "\n",
    "        # Assign estimate as a majority vote\n",
    "        estimate[i] =\n",
    "\n",
    "    return estimate\n",
    "\n",
    "est_labels = knn(X, y, 10)\n",
    "print(est_labels)\n",
    "\n",
    "# Let us compute the training error\n",
    "train_error =\n",
    "print(train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see the solution\n",
    "# %load knn2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first half of the vector should be composed solely of 0's, and the second half should be composed of 1's; we can see however that there are some mistakes. Let us plot them to try to understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.plot(X[y == 0, 0], X[y == 0, 1], \"o\", markersize=8)\n",
    "plt.plot(X[y == 1, 0], X[y == 1, 1], \"x\", markersize=8)\n",
    "\n",
    "# Draw a red circle around misclassified samples\n",
    "errors = (y != est_labels)\n",
    "plt.plot(X[errors, 0], X[errors, 1], \"o\", color=\"red\", markeredgewidth=3, markerfacecolor=\"white\", markersize=12, alpha=0.5)\n",
    "#plt.plot(X[errors, 0], X[errors, 1], \"o\", color=\"red\", mew=3, mfc=\"white\", ms=12, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, mistakes happen in regions where the majority of points belong to the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs. generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens when we try to classify points outside the training set? There are a few different ways of assessing this. For instance, we could have used only part of our data in the training set (say around 80%), and use the remaining to compute the so-called test error.\n",
    "Since in our case however the generative model is known, we might as well just get more samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample test data from the model\n",
    "testsamples_per_class = 10000\n",
    "\n",
    "class0_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
    "class1_testlabels = np.random.randint(10, size = testsamples_per_class)\n",
    "class0_testsamples = class0_centroids[class0_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
    "class1_testsamples = class1_centroids[class1_testlabels, :] + np.sqrt(1. / 5) * np.random.randn(testsamples_per_class, 2)\n",
    "\n",
    "X_test = np.vstack((class0_testsamples, class1_testsamples))\n",
    "y_test = np.hstack((np.zeros(testsamples_per_class), np.ones(testsamples_per_class)))\n",
    "\n",
    "# Compute distance matrix between X and X_test\n",
    "distances_test = cdist(X, X_test)\n",
    "print(distances_test)\n",
    "print(np.shape(distances_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test data\n",
    "plt.plot(X_test[y_test == 0, 0], X_test[y_test == 0, 1], \"o\", ms=8)\n",
    "plt.plot(X_test[y_test == 1, 0], X_test[y_test == 1, 1], \"x\", ms=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a function similar to `knn` that computes the estimates not for the points on the training set, but for points on a new *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_test(X_train, y_train, X_test, y_test, k):\n",
    "    n_train, n_test = len(y_train), len(y_test)\n",
    "    distances = cdist(X_train, X_test)\n",
    "    estimate = np.zeros(n_test)\n",
    "    \n",
    "    # For each sample in the test set...\n",
    "    for i in range(n_test):\n",
    "        # Look up k closest samples in the training set\n",
    "        nns = np.argpartition(distances[:, i], k)[:k]\n",
    "        \n",
    "        # Assign estimate as a majority vote\n",
    "        estimate[i] = int((sum(y_train[nns] == 1)) > sum(y_train[nns] == 0))\n",
    "\n",
    "    return estimate\n",
    "\n",
    "est_testlabels = knn_test(X, y, X_test, y_test, 10)\n",
    "print(est_testlabels)\n",
    "\n",
    "# Let us compute the test error, now\n",
    "print(np.mean(y_test != est_testlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem that we have when using $k$-nearest neighbors is that we have to pick a value for $k$ -- it is not clear in principle how to do it! \n",
    "In order to understand this better, let us look at how the training and test errors behave as a function of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience functions that compute the training and test errors, given training and test samples\n",
    "def compute_train_error(X, y, k=1):\n",
    "    y_hat = knn(X, y, k)\n",
    "    return np.mean(y != y_hat)\n",
    "    \n",
    "def compute_test_error(X_train, y_train, X_test, y_test, k=1):\n",
    "    y_hat = knn_test(X_train, y_train, X_test, y_test, k)\n",
    "    return np.mean(y_test != y_hat)\n",
    "\n",
    "# Run functions for k belonging to a range of values\n",
    "ks = np.arange(1, 20)\n",
    "train_error = []\n",
    "test_error = []\n",
    "for (i, k) in enumerate(ks):\n",
    "    train_error.append(compute_train_error(X, y, k))\n",
    "    test_error.append(compute_test_error(X, y, X_test, y_test, k))\n",
    "    print(\"k = %d; train error = %g, test error = %g\" % (k, train_error[-1], test_error[-1]))\n",
    "\n",
    "# Plot results\n",
    "plt.plot(ks, train_error, label = \"train\")\n",
    "plt.plot(ks, test_error, label = \"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$k$\")\n",
    "plt.ylabel(\"misclassification error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to use another quantity in the x axis instead of $k$: the number of degrees of freedom $N / k$. Indeed, the larger the $k$, the smaller the number of effective parameters -- think for instance of the $k = N$ limit, where everyone is assigned the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the experiment above for a different range of values\n",
    "ks = np.r_[np.arange(1, 10), np.arange(10, 150, 30)]\n",
    "train_error = []\n",
    "test_error = []\n",
    "for k in ks:\n",
    "    train_error.append(compute_train_error(X, y, k))\n",
    "    test_error.append(compute_test_error(X, y, X_test, y_test, k))\n",
    "    print(\"k = %d; train error = %g, test error = %g\" % (k, train_error[-1], test_error[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error as a function of the degrees of freedom\n",
    "plt.plot(len(y) / np.array(ks), train_error, \"-o\", label = \"train\")\n",
    "plt.plot(len(y) / np.array(ks), test_error, \"-o\", label = \"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"degrees of freedom $N / k$\")\n",
    "plt.ylabel(\"misclassification error\")\n",
    "plt.ylim((0.025, 0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot makes explicit the so-called *bias-variance tradeoff* that appears all throughout statistics. If we have more parameters, we are able to get smaller training error, but the test (analogously, generalization) error actually increases, meaning we are overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Python library for Machine Learning: scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that running our implementation of $k$-NNs for 20000 samples already started to feel a bit sluggish... That is because it isn't really optimized. If we wanted things to be fast, we would have to recur to some C code, using Python extensions such as Cython or ctypes (that's actually what packages such as Numpy and Scipy do for us).\n",
    "Moreover, we did things very naively -- for instance, one actually does not need to compute the full distance matrix if using appropriate data structures such as kd-trees etc.\n",
    "\n",
    "Luckily for us there are people who already wrote optimized versions of most standard machine learning algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scikit-learn website](sklearn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's see how scikit-learn's k-NN implementation works. One nice thing about scikit-learn is that they have lots of examples available online, so we can just look for something similar to what we are trying to do. For k-NN classification in particular there's this: http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier(10)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And done! With just three lines of code we were able to repeat everything we've been doing so far. Let us see if we get the same training error as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = np.mean(y != clf.predict(X))\n",
    "test_error = np.mean(y_test != clf.predict(X_test))\n",
    "#train_error = 1. - clf.score(X, y)\n",
    "#test_error = 1. - clf.score(X_test, y_test)\n",
    "\n",
    "print(\"train/test error (for k = 10): %g/%g\" % (train_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since things are optimized here, we can even do cooler things such as plotting the actual decision boundaries. In order to do that, let us generate a grid and compute the estimate for each point in that grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 101), np.linspace(-5, 5, 101))\n",
    "zz = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Do some plotting\n",
    "zz = zz.reshape(xx.shape)\n",
    "plt.imshow(zz, interpolation=\"nearest\", cmap=\"coolwarm\", alpha=0.2, origin=\"lower\", extent=[xx.min(), xx.max(), yy.min(), yy.max()])\n",
    "plt.plot(X[y == 0, 0], X[y == 0, 1], \"o\", markersize=8)\n",
    "plt.plot(X[y == 1, 0], X[y == 1, 1], \"x\", markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the best value of $k$ by cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of determining the \"best\" value of $k$ is by partitioning our dataset into two (training and validation sets) and then adjusting its value so as to minimize the validation error (we then need a 3rd set where we can compute the test error -- never adjust your parameters and compute the test error using the same set!)\n",
    "\n",
    "However, this does not provide us good statistics since the partitioning is only done once; a perhaps better way of doing it is by partitioning the dataset again and again, and recomputing the score. This is the idea behind cross-validation.\n",
    "\n",
    "scikit-learn has nice convenience functions to perform cross-validation for us. We gonna use `GridSearchCV`, which trains an estimator (in this case, the `KNeighborsClassifier`) across a range of parameters. For each of these parameters, the test error is computed using cross validation, more precisely 3-fold cross validation: the dataset is divided in 3, and all 3 combinations of 1 test set + 2 training sets are tried, with the resulting test error given by the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the grid search\n",
    "parameters = [{'n_neighbors': np.arange(1, 20)}]\n",
    "clf = GridSearchCV(neighbors.KNeighborsClassifier(n_neighbors = 1), parameters)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Print results\n",
    "print(clf.best_params_)\n",
    "print(clf.cv_results_[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this does on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = np.mean(y != clf.predict(X))\n",
    "test_error = np.mean(y_test != clf.predict(X_test))\n",
    "print(\"train/test error (for optimal k): %g/%g\" % (train_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on real datasets: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us work with a real dataset! If you have already studied a bit of machine learning and statistics, you are probably familiar with the MNIST dataset of handwritten digits. It is one of the most famous datasets used for benchmarking clustering and classification algorithms.\n",
    "\n",
    "It is also included in scikit-learn. Let's see how to get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home = \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data to see how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data\n",
    "y = mnist.target\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "n_samples, n_features = np.shape(X)\n",
    "\n",
    "# Plot a sample\n",
    "plt.imshow(X[20000, :].reshape((int(np.sqrt(n_features)), -1)), cmap=\"gray\")\n",
    "\n",
    "# Partition set into train/test\n",
    "samples = np.random.randint(60000, size = 1000)\n",
    "X_train, y_train = X[samples, :], y[samples]\n",
    "X_test, y_test = X[60000:, :], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_error = 1. - clf.score(X_train, y_train)\n",
    "test_error = 1. - clf.score(X_test, y_test)\n",
    "print(\"train/test error: %g/%g\" % (train_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good but not so much... It's quite slow, to start with. \n",
    "\n",
    "Let us look at the misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.where(y_train != clf.predict(X_train))[0]\n",
    "print(errors)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].imshow(X_train[errors[0], :].reshape((int(np.sqrt(n_features)), -1)), cmap=\"gray\")\n",
    "axs[0, 0].set_title(\"predicted label: %d\" % (clf.predict(X_train[errors[0], :].reshape(1, -1))))\n",
    "axs[0, 1].imshow(X_train[errors[1], :].reshape((int(np.sqrt(n_features)), -1)), cmap=\"gray\")\n",
    "axs[0, 1].set_title(\"predicted label: %d\" % (clf.predict(X_train[errors[1], :].reshape(1, -1))))\n",
    "axs[1, 0].imshow(X_train[errors[2], :].reshape((int(np.sqrt(n_features)), -1)), cmap=\"gray\")\n",
    "axs[1, 0].set_title(\"predicted label: %d\" % (clf.predict(X_train[errors[2], :].reshape(1, -1))))\n",
    "axs[1, 1].imshow(X_train[errors[3], :].reshape((int(np.sqrt(n_features)), -1)), cmap=\"gray\")\n",
    "axs[1, 1].set_title(\"predicted label: %d\" % (clf.predict(X_train[errors[3], :].reshape(1, -1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we ran $k$-NN on only two dimensions; now our vectors are much larger, they actually have 784 dimensions. It is expected that local methods such as $k$-NN don't work as well once we have gone to large dimensions. Why is that?\n",
    "\n",
    "Tip: note that the amount of samples needed to cover a certain fraction of the space grows exponentially with the dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

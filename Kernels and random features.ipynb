{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Features and Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random linear features: Johnson-Lindenstrauss lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly speaking, the JL lemma guarantees that some maps preserve distance in an approximate way. More precisely\n",
    "\n",
    "> Given $0 < \\varepsilon < 1$, a set $X$ of $m$ points in $\\mathbb{R}^N$, and a number $n > 8 \\log(m) / \\varepsilon^2$, there is a linear map $f: \\mathbb{R}^N \\to \\mathbb{R}^n$ such that\n",
    ">\n",
    ">$$(1 - \\varepsilon) \\|u - v\\|^2 \\leq \\|f(u) - f(v)\\|^2 \\leq (1 + \\varepsilon) \\|u - v\\|^2$$\n",
    ">\n",
    "> for all $u, v \\in X$.\n",
    "\n",
    "Moreover, we actually know how to construct such maps; for instance, we know that \n",
    "\n",
    "$$f(x) = \\frac{1}{\\sqrt{n}} A x, \\qquad A_{ij} \\sim \\mathcal{N} (0, 1)$$\n",
    "\n",
    "proves the lemma. Let us see how this works in practice! We first load some dataset, say the Boston dataset of house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dset = datasets.load_boston()\n",
    "print(dset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly small dataset, it should allows us to run things fast. Let's split the dataset in training/test sets, and do some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dset.data, dset.target\n",
    "print(\"number of samples/features: %d, %d\" % X.shape)\n",
    "\n",
    "# Do train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "print(\"samples/features in training set: %d, %d\" % X_train.shape)\n",
    "\n",
    "# Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the map mentioned above. We gonna generate $K$ new features by performing linear combinations of the features at random, with coefficients sampled from a standard Normal distribution. Our new set of samples is then gonna contain $N$ samples, each of dimension $K$ (instead of $P$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_features(X, n_projections):\n",
    "    n_features = X.shape[1]\n",
    "    W = np.random.randn(n_features, n_projections) / np.sqrt(n_projections)\n",
    "    return X.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new set of samples by mapping from 30 features to 2\n",
    "X_transf = generate_random_features(X_train, 2)\n",
    "\n",
    "# Plot Gram matrix ...\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 6))\n",
    "\n",
    "# ... for the original set of samples ...\n",
    "p0 = axs[0].imshow(X_train.dot(X_train.T), interpolation=\"nearest\", vmin = -50, vmax = 50)\n",
    "plt.colorbar(p0, ax=axs[0], shrink = 0.75)\n",
    "\n",
    "# ... and for the transformed one.\n",
    "p1 = axs[1].imshow(X_transf.dot(X_transf.T), interpolation=\"nearest\", vmin = -50, vmax = 50)\n",
    "plt.colorbar(p1, ax=axs[1], shrink = 0.75)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By playing a bit with the number of features in the transformed space, we can see how the Gramiam evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to see how the distance between points change as we change the number of features in the transformed space.\n",
    "\n",
    "**Exercise**: plot 2D histogram of distances between samples in original and transformed space. Tip: take a look at the `hist2d` function in Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Compute distance matrix for both original and transformed sets of features\n",
    "dists_orig =\n",
    "dists_proj =\n",
    "\n",
    "# Plot 2D histogram of distances between samples in original/transformed spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to see the solution to the exericse\n",
    "#%load rks1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random non-linear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now replace $\\langle{\\bf x}_i, {\\bf x}_j\\rangle$ by a kernel function $K({\\bf x}_i, {\\bf x}_j)$, more specifically by the RBF kernel $K({\\bf x}_i, {\\bf x}_j) = e^{-\\frac{1}{2} \\| {\\bf x}_i - {\\bf x}_j \\|^2_2}$. This essentially take us to infinite dimension!\n",
    "\n",
    "**Exercise**: compute and plot the kernelized Gram matrix for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gram matrix using the original kernel ...\n",
    "from scipy.spatial.distance import cdist\n",
    "gram_orig =\n",
    "\n",
    "# ... and plot it.\n",
    "plt.imshow(gram_orig, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to see the solution\n",
    "#%load rks2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, it's often more efficient to *approximate* the kernel by expanding it in a given basis (usually Fourier) and then approximately performing the integral via importance sampling. This provides us with a map \n",
    "\n",
    "$$\\phi({\\bf x}_i, {\\bf w}) = e^{i {\\bf w} \\cdot {\\bf x}_i}$$\n",
    "\n",
    "such that \n",
    "\n",
    "$$K({\\bf x}_i, {\\bf x}_j) \\approx \\sum_{{\\bf w} \\sim \\mathcal{N} (0, I_P)} \\phi({\\bf x}_i, {\\bf w}) \\phi({\\bf x}_j, {\\bf w})$$\n",
    "\n",
    "This is the so-called *random kitchen sinks* algorithm. Let's implement it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RKS](rks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_features(X, n_projections):\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Sample w\n",
    "    w =\n",
    "    \n",
    "    # Compute z\n",
    "    z =\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to see the solution to the exercise\n",
    "#%load rks3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the approximated Gram matrix approaches the exact one as `n_projections` increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Gram matrix using the approximate kernel\n",
    "X_transf = generate_nonlinear_features(X_train, 2)\n",
    "gram_approx = X_transf.dot(X_transf.T)\n",
    "\n",
    "# Plot Gram matrices using both exact and approximate kernels\n",
    "fig, axs = plt.subplots(1, 2, figsize = (12, 6))\n",
    "p0 = axs[0].imshow(gram_orig, interpolation=\"nearest\", vmin = 0, vmax = 0.2)\n",
    "p1 = axs[1].imshow(gram_approx, interpolation=\"nearest\", vmin = 0, vmax = 0.2)\n",
    "plt.colorbar(p0, ax=axs[0], shrink = 0.75)\n",
    "plt.colorbar(p1, ax=axs[1], shrink = 0.75)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many projections do we need before $\\phi (x) \\phi(\\mu)$ gives a good approximation to $K(x, \\mu) = e^{-\\frac{(x - \\mu)^2}{2}}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1.3\n",
    "\n",
    "# Generate 1D grid\n",
    "xx = np.linspace(-5, 5, 101)\n",
    "\n",
    "# Compute kernel approximation\n",
    "def compute_approx(grid, n_projections):\n",
    "    phi = generate_nonlinear_features((grid - mu).reshape(-1, 1), n_projections)\n",
    "    return np.sum(phi, 1) / np.sqrt(n_projections)\n",
    "\n",
    "# Plot approximation and exact kernels\n",
    "plt.plot(xx, compute_approx(xx, 100))\n",
    "plt.plot(xx, np.exp(-.5 * (xx - mu) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our kernel approximation perform in the solution of linear models. We first start with doing ridge regression while replacing the Gram matrix by the (exact) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kernel_ridge(X, y, lamb = 0.05):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Obtain kernelized Gram matrix and perform least-squares estimation\n",
    "    corr =\n",
    "    coef =\n",
    "    \n",
    "    # Compute training error\n",
    "    error = np.mean((y - corr.T.dot(coef)))\n",
    "    print(\"error on training set: %g\" % error)\n",
    "    \n",
    "    return coef\n",
    "\n",
    "def test_kernel_ridge(X_test, y_test, X_train, coef):\n",
    "    corr =\n",
    "    \n",
    "    # Compute test (generalization) error\n",
    "    error = np.mean((y_test - corr.T.dot(coef)) ** 2)\n",
    "    print(\"error on test set: %g\" % error)\n",
    "    \n",
    "    return error\n",
    "\n",
    "coef = train_kernel_ridge(X_train, y_train)\n",
    "kernel_ridge_error = test_kernel_ridge(X_test, y_test, X_train, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to see the solution to the exercise\n",
    "#%load rks4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replace the exact kernel with the approximate one. We first rewrite the `generate_nonlinear_features` function so as to receive/return $\\omega$ values; this is necessary in order to compute $K$ for new points outside the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_features(X, n_projections, w = None):\n",
    "    n_features = X.shape[1]\n",
    "    if w is None:\n",
    "        w = np.random.randn(n_features, n_projections)\n",
    "    z = np.hstack((np.cos(X.dot(w)), np.sin(X.dot(w)))) / np.sqrt(n_projections)\n",
    "    return z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to rewrite the functions above using approximate kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rks(X, y, n_projections, lamb = 0.05):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Obtain random features and perform least-squares estimation\n",
    "    z, w = generate_nonlinear_features(X, n_projections)\n",
    "    coef = z.T.dot(np.linalg.lstsq(z.dot(z.T) + lamb * np.eye(n_samples), y)[0])\n",
    "    \n",
    "    # Compute training error\n",
    "    error = np.mean((y - z.dot(coef)))\n",
    "    print(\"error on training set: %g\" % error)\n",
    "    \n",
    "    return coef, w\n",
    "\n",
    "def test_rks(X, y, coef, w):\n",
    "    z, _ = generate_nonlinear_features(X, w.shape[1], w)\n",
    "    \n",
    "    # Compute test (generalization) error\n",
    "    error = np.mean((y_test - z.dot(coef)) ** 2)\n",
    "    print(\"error on test set: %g\" % error)\n",
    "    \n",
    "    return error\n",
    "\n",
    "coef, w = train_rks(X_train, y_train, 500)\n",
    "test_rks(X_test, y_test, coef, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the performance as a function of `n_projections`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test error for many values of n_projections\n",
    "ks = np.arange(200, 5000, 200)\n",
    "errors = np.zeros(len(ks))\n",
    "for (i, k) in enumerate(ks):\n",
    "    print(\"\\nn_projections = %d\" % (k))\n",
    "    coef, w = train_rks(X_train, y_train, k)\n",
    "    errors[i] = test_rks(X_test, y_test, coef, w)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(ks, errors)\n",
    "plt.axhline(kernel_ridge_error, c=\"k\", ls=\"-.\", lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We didn't gain so much in using RKS in the previous section, because the number of samples we had was very small. Let us try it again with a bigger dataset.\n",
    "\n",
    "Moreover, we will use Support Vector Machines, which requires us to evaluate the kernel multiple times and not only once as in ridge regression. That makes even more important to optimize the computation.\n",
    "\n",
    "We start by loading the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata(\"MNIST original\", data_home=\"../lec1\")\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Preprocess\n",
    "X = X / 255.\n",
    "\n",
    "mask = np.logical_or.reduce([(y == k) for k in [0, 1]])\n",
    "X, y = X[mask, :], y[mask]\n",
    "\n",
    "n_samples = 5000\n",
    "samples = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "X, y = X[samples, :], y[samples]\n",
    "\n",
    "# Do train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "print(\"samples/features in training set: %d, %d\" % X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run the kernel SVM function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import svm\n",
    "\n",
    "# Create kernel SVM\n",
    "kernel_svm = svm.SVC(gamma=.2)\n",
    "\n",
    "# Fit kernel SVM\n",
    "kernel_svm_time = time()\n",
    "kernel_svm.fit(X_train, y_train)\n",
    "kernel_svm_error = np.mean(y_test != kernel_svm.predict(X_test))\n",
    "kernel_svm_time = time() - kernel_svm_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we repeat the process for the Fourier-approximated kernel SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "# Create Fourier-approximated SVM\n",
    "map_fourier = RBFSampler(gamma=.2)\n",
    "fourier_svm = pipeline.Pipeline([(\"feature_map\", map_fourier),\n",
    "                                 (\"svm\", svm.LinearSVC())])\n",
    "\n",
    "# Fit Fourier-approximated SVM\n",
    "fourier_times = []\n",
    "fourier_errors = []\n",
    "\n",
    "sample_sizes = [100, 200, 500, 1000, 2000, 5000]\n",
    "for k in sample_sizes:\n",
    "    fourier_svm.set_params(feature_map__n_components=k)\n",
    "    \n",
    "    start = time()\n",
    "    fourier_svm.fit(X_train, y_train)\n",
    "    fourier_times.append(time() - start)\n",
    "    fourier_error = np.mean(y_test != fourier_svm.predict(X_test))\n",
    "    fourier_errors.append(fourier_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "axs[0].plot(sample_sizes, fourier_errors)\n",
    "axs[0].axhline(kernel_svm_error, c=\"k\", ls=\"-.\", lw=2)\n",
    "axs[0].set_ylabel(\"error\")\n",
    "axs[1].plot(sample_sizes, fourier_times)\n",
    "axs[1].axhline(kernel_svm_time, c=\"k\", ls=\"-.\", lw=2)\n",
    "axs[1].set_ylabel(\"runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
